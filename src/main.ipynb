{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Group Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing , Cleaning and Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer,LancasterStemmer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import Word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "def cleaning(dataframe):\n",
    "    lowercase(dataframe)\n",
    "    replace_email(dataframe)\n",
    "    remove_symbols(dataframe)\n",
    "    replace_numbers(dataframe)\n",
    "    remove_gibberish(dataframe)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lowercase(dataframe):\n",
    "    dataframe['Article'] = dataframe['Article'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_email(dataframe):\n",
    "    regex = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "    # dataframe['Article'] = dataframe['Article'].apply(lambda x: (re.sub(regex, \" \", x)))\n",
    "    dataframe['Article'] = dataframe['Article'].apply(lambda x: (re.sub(regex, \"emailadd\", x)))\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_symbols(dataframe):\n",
    "    # regex = r'[\\p{P}\\p{S}]'\n",
    "    # regex = r'\\W+'\n",
    "    regex = r'[^a-zA-Z0-9]+'\n",
    "    dataframe['Article'] = dataframe['Article'].apply(lambda x: re.sub(regex,\" \",x))\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def replace_numbers(dataframe):\n",
    "    regex = r'\\d+[\\.\\-\\d]*'\n",
    "    dataframe['Article'] = dataframe['Article'].apply(lambda x: re.sub(regex,\"numb\",x))\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_gibberish(dataframe):\n",
    "    nltk.download('words')\n",
    "    words = set(nltk.corpus.words.words())\n",
    "    dataframe['Article'] = dataframe['Article'].apply(lambda x: \" \".join(w for w in nltk.wordpunct_tokenize(x) if w in words or not w.isalpha()))\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain of functions for preprocessing\n",
    "def preprocessing(dataframe):\n",
    "    # cleaning\n",
    "    dataframe = cleaning(dataframe)\n",
    "    # preprocesing\n",
    "    dataframe =  remove_stopWords(dataframe)\n",
    "    # lemmatize(dataframe)\n",
    "\n",
    "    # Stemmer(dataframe, PorterStemmer())\n",
    "    # Stemmer(dataframe, LancasterStemmer())\n",
    "    dataframe =  Stemmer(dataframe, SnowballStemmer(\"english\"))\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_stopWords(dataframe):\n",
    "    nltk.download('stopwords')\n",
    "    stop = stopwords.words('english')\n",
    "    dataframe['Article'] = dataframe['Article'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Stemmer(dataframe,st):\n",
    "    dataframe['Article'] = dataframe['Article'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lemmatize(dataframe):\n",
    "    dataframe['Article'] = dataframe['Article'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(train_x,valid_x): \n",
    "    tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w+')\n",
    "    tfidf_vect.fit(df['Article'])\n",
    "    xtrain = tfidf_vect.transform(train_x)\n",
    "    xtest = tfidf_vect.transform(valid_x)\n",
    "    return xtrain,xtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from preprocessing import preprocess\n",
    "from sklearn import preprocessing\n",
    "# from model1 import linear\n",
    "# from model2 import naive, naive_bayes\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics\n",
    "\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Paths = {\n",
    "    \"data\": r\"../Data/20news-18828\",  # contains subdirectories!\n",
    "    \"data_train\": r\"../Data/train\",  # contains subdirectories!\n",
    "    \"data_test\": r\"../Data/test\",  # contains subdirectories!\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(path):\n",
    "    dataframe = pd.DataFrame(columns=[\"Label\", \"Article\"])\n",
    "    dirs = os.listdir(path)\n",
    "    for dir in dirs:\n",
    "        for file in os.listdir(os.path.join(path, dir)):\n",
    "            with open(os.path.join(path, dir, file)) as f:\n",
    "                row = pd.DataFrame([{\"Label\": dir, \"Article\": f.read()}])\n",
    "                dataframe = pd.concat([dataframe, row])\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_data(path):\n",
    "    dataframe = load_data(path)\n",
    "    dataframe = dataframe.sample(frac=1, random_state=45).reset_index()\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid,\n",
    "                valid_y):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_y(ytrain, ytest):\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    ytrain = encoder.fit_transform(ytrain)\n",
    "    ytest = encoder.fit_transform(ytest)\n",
    "    return ytrain,ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7837263649883153\n",
      "Accuracy:  0.8253664754620778\n"
     ]
    }
   ],
   "source": [
    "df = read_data(Paths[\"data\"])\n",
    "# df = pd.read_csv(\"../Data/data.csv\")\n",
    "# df = df.sample(frac=1, random_state=1).reset_index()\n",
    "\n",
    "df.to_csv(\"../Data/data.csv\", index=False)\n",
    "\n",
    "preprocess(df)\n",
    "\n",
    "# data split\n",
    "train_x, valid_x, ytrain, ytest = model_selection.train_test_split(\n",
    "    df['Article'], df['Label'])\n",
    "\n",
    "encode_y(ytrain, ytest)\n",
    "\n",
    "xtrain,xtest = extract_features(train_x,valid_x)\n",
    "\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(alpha=1e-10), xtrain,\n",
    "                        ytrain, xtest, ytest)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "accuracy = train_model(linear_model.LogisticRegression(max_iter=1000),\n",
    "                        xtrain, ytrain, xtest, ytest)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3c53d38db13075794741e8371862ca73ad19b2b8ffba1432ab561e98420da23f"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
